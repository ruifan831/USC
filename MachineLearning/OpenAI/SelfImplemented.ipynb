{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "while True: \n",
    "    action=env.action_space.sample()\n",
    "    observation, reward, done, info=env.step(action)# take a random action\n",
    "    \n",
    "    print(done)\n",
    "    if done:\n",
    "        print(done)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "class dql_model():\n",
    "    def __init__(self,action_shape,state_shape):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.action_shape=action_shape\n",
    "        self.state_shape=state_shape\n",
    "        self.model=Sequential()\n",
    "        self.model.add(Dense(24,input_dim=self.state_shape,activation='relu'))\n",
    "        self.model.add(Dense(24,activation='relu'))\n",
    "        self.model.add(Dense(24,activation='relu'))\n",
    "        self.model.add(Dense(self.action_shape,activation='linear'))\n",
    "        \n",
    "        opt=tf.keras.optimizers.Adam(lr=0.001,decay=1e-6)\n",
    "\n",
    "        self.model.compile(loss='mse',optimizer=opt)\n",
    "\n",
    "    def memory_replay(self,memory):\n",
    "        batch_size=20\n",
    "        if len(memory)< batch_size:\n",
    "            return\n",
    "        batch= random.sample(memory,batch_size)\n",
    "        for state,next_state, reward, done, info,action in batch:\n",
    "            q_update= reward\n",
    "            if not done:\n",
    "                q_update= reward + 0.9 * np.amax(self.model.predict(next_state)[0])\n",
    "            q_values=self.model.predict(state)\n",
    "            q_values[0][action]= q_update\n",
    "            self.model.fit(state,q_values,verbose=0)\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "            \n",
    "    def action_decision(self,state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_shape)\n",
    "        action=np.argmax(self.model.predict(state)[0])\n",
    "        return action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished\n",
      "Episode 1 finished\n",
      "Episode 2 finished\n",
      "Episode 3 finished\n",
      "Episode 4 finished\n",
      "Episode 5 finished\n",
      "Episode 6 finished\n",
      "Episode 7 finished\n",
      "Episode 8 finished\n",
      "Episode 9 finished\n",
      "Episode 10 finished\n",
      "Episode 11 finished\n",
      "Episode 12 finished\n",
      "Episode 13 finished\n",
      "Episode 14 finished\n",
      "Episode 15 finished\n",
      "Episode 16 finished\n",
      "Episode 17 finished\n",
      "Episode 18 finished\n",
      "Episode 19 finished\n",
      "Episode 20 finished\n",
      "Episode 21 finished\n",
      "Episode 22 finished\n",
      "Episode 23 finished\n",
      "Episode 24 finished\n",
      "Episode 25 finished\n",
      "Episode 26 finished\n",
      "Episode 27 finished\n",
      "Episode 28 finished\n",
      "Episode 29 finished\n",
      "Episode 30 finished\n",
      "Episode 31 finished\n",
      "Episode 32 finished\n",
      "Episode 33 finished\n",
      "Episode 34 finished\n",
      "Episode 35 finished\n",
      "Episode 36 finished\n",
      "Episode 37 finished\n",
      "Episode 38 finished\n",
      "Episode 39 finished\n",
      "Episode 40 finished\n",
      "Episode 41 finished\n",
      "Episode 42 finished\n",
      "Episode 43 finished\n",
      "Episode 44 finished\n",
      "Episode 45 finished\n",
      "Episode 46 finished\n",
      "Episode 47 finished\n",
      "Episode 48 finished\n",
      "Episode 49 finished\n",
      "Episode 50 finished\n",
      "Episode 51 finished\n",
      "Episode 52 finished\n",
      "Episode 53 finished\n",
      "Episode 54 finished\n",
      "Episode 55 finished\n",
      "Episode 56 finished\n",
      "Episode 57 finished\n",
      "Episode 58 finished\n",
      "Episode 59 finished\n",
      "Episode 60 finished\n",
      "Episode 61 finished\n",
      "Episode 62 finished\n",
      "Episode 63 finished\n",
      "Episode 64 finished\n",
      "Episode 65 finished\n",
      "Episode 66 finished\n",
      "Episode 67 finished\n",
      "Episode 68 finished\n",
      "Episode 69 finished\n",
      "Episode 70 finished\n",
      "Episode 71 finished\n",
      "Episode 72 finished\n",
      "Episode 73 finished\n",
      "Episode 74 finished\n",
      "Episode 75 finished\n",
      "Episode 76 finished\n",
      "Episode 77 finished\n",
      "Episode 78 finished\n",
      "Episode 79 finished\n",
      "Episode 80 finished\n",
      "Episode 81 finished\n",
      "Episode 82 finished\n",
      "Episode 83 finished\n",
      "Episode 84 finished\n",
      "Episode 85 finished\n",
      "Episode 86 finished\n",
      "Episode 87 finished\n",
      "Episode 88 finished\n",
      "Episode 89 finished\n",
      "Episode 90 finished\n",
      "Episode 91 finished\n",
      "Episode 92 finished\n",
      "Episode 93 finished\n",
      "Episode 94 finished\n",
      "Episode 95 finished\n",
      "Episode 96 finished\n",
      "Episode 97 finished\n",
      "Episode 98 finished\n",
      "Episode 99 finished\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN=100\n",
    "memory=deque(maxlen=SEQ_LEN)\n",
    "env = gym.make('CartPole-v0')\n",
    "dql=dql_model(env.action_space.n,4)\n",
    "for episode in range(100): \n",
    "    state=env.reset().reshape(1,-1)\n",
    "    while True:\n",
    "        action=dql.action_decision(state)\n",
    "        next_state, reward, done, info=env.step(action)# take a random action\n",
    "        next_state=next_state.reshape(1,-1)\n",
    "        memory.append([state,next_state, reward, done, info,action])\n",
    "        state=next_state\n",
    "        if done:\n",
    "            print(\"Episode {} finished\".format(episode))\n",
    "            break\n",
    "        dql.memory_replay(memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "for episode in range(10): \n",
    "    state=env.reset().reshape(1,-1)\n",
    "    env.render()\n",
    "    score_acc=0\n",
    "    while True:\n",
    "        action=dql.action_decision(state)\n",
    "        next_state, reward, done, info=env.step(action)# take a random action\n",
    "        next_state=next_state.reshape(1,-1)\n",
    "        if done:\n",
    "            reward=-reward\n",
    "        memory.append([state,next_state, reward, done, info,action])\n",
    "        state=next_state\n",
    "        score_acc+=reward\n",
    "        if done:\n",
    "            scores.append(score_acc)\n",
    "            break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.0, 7.0, 7.0, 8.0, 7.0, 8.0, 8.0, 8.0, 10.0, 7.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.429112, 11.220982]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state=env.reset().reshape(1,-1)\n",
    "dql.model.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
