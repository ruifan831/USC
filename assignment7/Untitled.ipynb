{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only call once\n",
    "def corpus():\n",
    "    appended_content=\"\"\n",
    "    for i in os.listdir():\n",
    "        if \".txt\" in i:\n",
    "            print(i)\n",
    "            file=open(i,\"r\",encoding=\"utf-8-sig\")\n",
    "            data=file.read().replace(\"\\n\",\"\")\n",
    "            appended_content=appended_content+data\n",
    "            file.close()\n",
    "    return appended_content\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mysticism and Logic and Other Essays.txt\n",
      "Our Knowledge of the External World as a Field for Scientific Method in.txt\n",
      "The Analysis of Mind.txt\n",
      "The Problems of Philosophy.txt\n"
     ]
    }
   ],
   "source": [
    "corpus_result=corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=[ord(i) for i in corpus_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "out_scaled=list(map(lambda x: x/255,list(filter(lambda x: x<255,out))))\n",
    "out_unscaled=list(filter(lambda x: x<255,out))\n",
    "W=100\n",
    "S=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "for i in range(len(out)):\n",
    "    try:\n",
    "        X.append(out_scaled[i:i+W-S])\n",
    "        Y.append(out_unscaled[i+W-S])\n",
    "    except IndexError:\n",
    "        break\n",
    "X=X[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.reshape(X,(-1,99,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.diag(np.ones(256))\n",
    "\n",
    "Y=np.array([A[i] for i in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,CuDNNLSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 3.0148 - acc: 0.1684\n",
      "Epoch 00001: loss improved from inf to 3.01478, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 376s 237us/sample - loss: 3.0148 - acc: 0.1684\n",
      "Epoch 2/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.8746 - acc: 0.1964\n",
      "Epoch 00002: loss improved from 3.01478 to 2.87456, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 375s 236us/sample - loss: 2.8746 - acc: 0.1964\n",
      "Epoch 3/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.8256 - acc: 0.2022\n",
      "Epoch 00003: loss improved from 2.87456 to 2.82560, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 374s 236us/sample - loss: 2.8256 - acc: 0.2022\n",
      "Epoch 4/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.7964 - acc: 0.2058\n",
      "Epoch 00004: loss improved from 2.82560 to 2.79642, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 371s 234us/sample - loss: 2.7964 - acc: 0.2058\n",
      "Epoch 5/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.7714 - acc: 0.2109\n",
      "Epoch 00005: loss improved from 2.79642 to 2.77136, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 372s 235us/sample - loss: 2.7714 - acc: 0.2109\n",
      "Epoch 6/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.7504 - acc: 0.2158\n",
      "Epoch 00006: loss improved from 2.77136 to 2.75036, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 372s 235us/sample - loss: 2.7504 - acc: 0.2158\n",
      "Epoch 7/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.7346 - acc: 0.2201\n",
      "Epoch 00007: loss improved from 2.75036 to 2.73460, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 376s 237us/sample - loss: 2.7346 - acc: 0.2201\n",
      "Epoch 8/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.7135 - acc: 0.2254\n",
      "Epoch 00008: loss improved from 2.73460 to 2.71354, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 398s 251us/sample - loss: 2.7135 - acc: 0.2254\n",
      "Epoch 9/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.6960 - acc: 0.2303\n",
      "Epoch 00009: loss improved from 2.71354 to 2.69597, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 394s 249us/sample - loss: 2.6960 - acc: 0.2303\n",
      "Epoch 10/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.6786 - acc: 0.2354\n",
      "Epoch 00010: loss improved from 2.69597 to 2.67859, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 384s 243us/sample - loss: 2.6786 - acc: 0.2354\n",
      "Epoch 11/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.6628 - acc: 0.2406\n",
      "Epoch 00011: loss improved from 2.67859 to 2.66275, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 394s 248us/sample - loss: 2.6628 - acc: 0.2406\n",
      "Epoch 12/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.6484 - acc: 0.2451\n",
      "Epoch 00012: loss improved from 2.66275 to 2.64840, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 397s 251us/sample - loss: 2.6484 - acc: 0.2451\n",
      "Epoch 13/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.6351 - acc: 0.2494\n",
      "Epoch 00013: loss improved from 2.64840 to 2.63514, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 392s 248us/sample - loss: 2.6351 - acc: 0.2494\n",
      "Epoch 14/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.6228 - acc: 0.2540\n",
      "Epoch 00014: loss improved from 2.63514 to 2.62282, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 375s 237us/sample - loss: 2.6228 - acc: 0.2540\n",
      "Epoch 15/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.6118 - acc: 0.2579\n",
      "Epoch 00015: loss improved from 2.62282 to 2.61177, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 376s 237us/sample - loss: 2.6118 - acc: 0.2579\n",
      "Epoch 16/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.6015 - acc: 0.2611\n",
      "Epoch 00016: loss improved from 2.61177 to 2.60147, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 376s 237us/sample - loss: 2.6015 - acc: 0.2611\n",
      "Epoch 17/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.5903 - acc: 0.2644\n",
      "Epoch 00017: loss improved from 2.60147 to 2.59034, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 376s 237us/sample - loss: 2.5903 - acc: 0.2644\n",
      "Epoch 18/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.5815 - acc: 0.2676\n",
      "Epoch 00018: loss improved from 2.59034 to 2.58146, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 375s 237us/sample - loss: 2.5815 - acc: 0.2676\n",
      "Epoch 19/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.5712 - acc: 0.2710\n",
      "Epoch 00019: loss improved from 2.58146 to 2.57118, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 374s 236us/sample - loss: 2.5712 - acc: 0.2710\n",
      "Epoch 20/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.5645 - acc: 0.2734\n",
      "Epoch 00020: loss improved from 2.57118 to 2.56453, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 374s 236us/sample - loss: 2.5645 - acc: 0.2734\n",
      "Epoch 21/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.5526 - acc: 0.2770\n",
      "Epoch 00021: loss improved from 2.56453 to 2.55263, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 374s 236us/sample - loss: 2.5526 - acc: 0.2770\n",
      "Epoch 22/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.5434 - acc: 0.2800\n",
      "Epoch 00022: loss improved from 2.55263 to 2.54336, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 375s 237us/sample - loss: 2.5434 - acc: 0.2800\n",
      "Epoch 23/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.5328 - acc: 0.2830\n",
      "Epoch 00023: loss improved from 2.54336 to 2.53281, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 373s 236us/sample - loss: 2.5328 - acc: 0.2831\n",
      "Epoch 24/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.5224 - acc: 0.2867\n",
      "Epoch 00024: loss improved from 2.53281 to 2.52239, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 374s 236us/sample - loss: 2.5224 - acc: 0.2867\n",
      "Epoch 25/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.5133 - acc: 0.2896\n",
      "Epoch 00025: loss improved from 2.52239 to 2.51328, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 374s 236us/sample - loss: 2.5133 - acc: 0.2896\n",
      "Epoch 26/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.5039 - acc: 0.2927\n",
      "Epoch 00026: loss improved from 2.51328 to 2.50392, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 374s 236us/sample - loss: 2.5039 - acc: 0.2927\n",
      "Epoch 27/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.4957 - acc: 0.2951\n",
      "Epoch 00027: loss improved from 2.50392 to 2.49570, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 372s 235us/sample - loss: 2.4957 - acc: 0.2951\n",
      "Epoch 28/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.4881 - acc: 0.2976\n",
      "Epoch 00028: loss improved from 2.49570 to 2.48813, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 375s 237us/sample - loss: 2.4881 - acc: 0.2976\n",
      "Epoch 29/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.4797 - acc: 0.2999\n",
      "Epoch 00029: loss improved from 2.48813 to 2.47968, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 375s 237us/sample - loss: 2.4797 - acc: 0.2999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "1584000/1584005 [============================>.] - ETA: 0s - loss: 2.4705 - acc: 0.3027\n",
      "Epoch 00030: loss improved from 2.47968 to 2.47054, saving model to weights.best.hdf5\n",
      "1584005/1584005 [==============================] - 374s 236us/sample - loss: 2.4705 - acc: 0.3027\n"
     ]
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(LSTM(256,input_shape=(99,1),activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(256, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='auto',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "hist=model.fit(X,Y,epochs=30,batch_size=1000,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
